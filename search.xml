<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>PyTorch Essential</title>
      <link href="/2023/03/21/pytorch-essential/"/>
      <url>/2023/03/21/pytorch-essential/</url>
      
        <content type="html"><![CDATA[<h1 id="1-PyTorch基礎知識"><a href="#1-PyTorch基礎知識" class="headerlink" title="1. PyTorch基礎知識"></a>1. PyTorch基礎知識</h1><pre class=" language-python="><code class="language-python=">import torch</code></pre><pre class=" language-python="><code class="language-python=">print(torch.__version__)  # 列印PyTorch版本a1 = torch.rand(3, 4)  # 建立3列4行的亂數矩陣print(a1.device, a1, sep='\n')# 如果系統支援GPU就將device = cudadevice = (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))print(device)  # print('cuda')代表可以使用GPU</code></pre><p><strong>1.10.0+cu113<br>cpu<br>tensor([[0.5499, 0.7371, 0.9104, 0.8024],<br>        [0.7628, 0.3287, 0.3319, 0.1342],<br>        [0.3354, 0.8344, 0.0088, 0.1546]])<br>cuda</strong></p><pre class=" language-python="><code class="language-python=">a2 = a1.to(device)  # 將張量由CPU移動到GPU，如果上面device有改變print(a1)print(a2)</code></pre><pre><code>tensor([[0.5499, 0.7371, 0.9104, 0.8024],        [0.7628, 0.3287, 0.3319, 0.1342],        [0.3354, 0.8344, 0.0088, 0.1546]])tensor([[0.5499, 0.7371, 0.9104, 0.8024],        [0.7628, 0.3287, 0.3319, 0.1342],        [0.3354, 0.8344, 0.0088, 0.1546]], device='cuda:0')</code></pre><p>張量(Tensor)是PyTorch的基礎運算單位，是一種資料結構，可以用來儲存及操作資料。<br>PyTorch的張量與NumPy的ndarray十分類似，但有兩個不同處:</p><ul><li>PyTorch的張量能利用GPU進行運算</li><li>PyTorch的張量在計算時，可以做為節點自動加入制計算圖中，而計算圖可以將其中的每個節點自動計算微分<br>PyTorch中，有兩種方法來建立張量:</li><li>使用torch.Tensor()</li><li>使用torch.tensor()</li></ul><pre class=" language-python="><code class="language-python=">a1 = torch.Tensor([1, 2])  # Tensor只會建立32位元浮點數的張量print(a1.type())print(a1)a1 = torch.tensor([[1.0, 2]])  # tensor會根據傳入數據建立對應張量print(a1.type())a1 = torch.tensor([1, 2])print(a1.type())a1 = torch.tensor([1, 2], dtype=torch.double)  # 使用tensor建立張量可以使用dtype指定型別print(a1.type())# dtype可以使用的型別# torch.float32或torch.float# torch.float64或torch.double# torch.float16或torch.half# torch.int16或torch.short# torch.int32或torch.int# torch.int64或torch.long# torch.int8# torch.uint8</code></pre><pre><code>torch.FloatTensortensor([1., 2.])torch.FloatTensortorch.LongTensortorch.DoubleTensor</code></pre><p>可以使用一些函式建立指定形狀的張量<br>torch.ones() 元素都為1的張量<br>torch.zeros() 元素都為0的張量<br>torch.rand() 隨機分布的張量，大小為[0, 1)<br>torch.randn() 標準常態分佈的張量，平均值為0，變異數為1</p><pre class=" language-python="><code class="language-python=">b1 = torch.zeros(2, 3)  # 建立2列3行的zero張量print(b1)b2 = torch.ones(2, 3)  # 建立2列3行的one張量print(b2)b3 = torch.rand(2, 3)  # 建立2列3行的rand張量print(b3)</code></pre><pre><code>tensor([[0., 0., 0.],        [0., 0., 0.]])tensor([[1., 1., 1.],        [1., 1., 1.]])tensor([[0.0399, 0.2605, 0.7716],        [0.6981, 0.7787, 0.2807]])</code></pre><pre class=" language-python="><code class="language-python="># torch.manual_seed()可以設定亂數種子。當執行torch.rand()時，每次都會得到不同的亂數張量，# 但通常在機器學習中，需要重現結果，在這個時候可以明確指定亂數種子，讓張量保持不變。torch.manual_seed(42)  # 設定亂數種子b4 = torch.rand(3, 3)print(b4)</code></pre><pre><code>tensor([[0.8823, 0.9150, 0.3829],        [0.9593, 0.3904, 0.6009],        [0.2566, 0.7936, 0.9408]])</code></pre><pre class=" language-python="><code class="language-python="># 張量的操作方法名稱後面帶有底線，如zero_()會直接些改張量內記憶體內的值。b5 = torch.ones(3, 2)b6 = b5.zero_()print(b5)# 若張量中只有1個元素，可以使用item()，將張量變純量。c1 = torch.tensor([2])c2 = c1.item()print(c2)# 其他都可以，c2轉成了純量print(type(b6), type(c1), type(c2))# 將指令張量資料型別轉變# 方法一c3 = torch.ones(2, 3, dtype=torch.double)print(c3.type())# 方法二c3 = torch.ones(2, 3).to(torch.double)print(c3.type())# 方法三c3 = torch.ones(2, 3).double()print(c3.type())</code></pre><pre><code>tensor([[0., 0.],        [0., 0.],        [0., 0.]])2&lt;class 'torch.Tensor'&gt; &lt;class 'torch.Tensor'&gt; &lt;class 'int'&gt;torch.DoubleTensortorch.DoubleTensortorch.DoubleTensor</code></pre><h1 id="2-張量與numpy"><a href="#2-張量與numpy" class="headerlink" title="2. 張量與numpy"></a>2. 張量與numpy</h1><p>PyTorch支援將張量轉成NumPy，只要執行Tensor.numpy()就可以將張量轉成NumPy的ndarray資料型態。</p><pre class=" language-python="><code class="language-python=">d1 = torch.ones(2, 3)d1_np = d1.numpy()print(type(d1_np), type(d1), sep='\n')</code></pre><pre><code>&lt;class 'numpy.ndarray'&gt;&lt;class 'torch.Tensor'&gt;</code></pre><p>若要使用NumPy來建立PyTorch張量，可以執行torch.from_numpy()。</p><pre class=" language-python="><code class="language-python=">import numpy as npd2_np = np.array([1, 2, 3])d2 = torch.from_numpy(d2_np)print(type(d2))</code></pre><pre><code>&lt;class 'torch.Tensor'&gt;</code></pre><p>PyTorch預設會將張量定義再CPU控制的記憶體中，如果要使用GPU加速運算，可以判別系統是否支援GPU，並且在建立張量時指定device屬性。</p><pre class=" language-python="><code class="language-python=">device = torch.device("cuda" if torch.cuda.is_available() else "cpu")print(device)# x和y都有指定device，所以如果有偵測到GPU存在，會將張量定義再GPU控制的記憶體中x = torch.tensor([[1, 2, 3], [4, 5, 6]], device=device)y = torch.tensor([[7, 8, 9], [10, 11, 12]], device=device)z = x + y  # x跟y都是在GPU記憶體中，所以z也是(x跟y都要在同一個device中，不然會報錯)print(z)</code></pre><pre><code>cudatensor([[ 8, 10, 12],        [14, 16, 18]], device='cuda:0')</code></pre><p>也可以使用Tensor.to()讓張量在CPU和GPU中移動</p><pre class=" language-python="><code class="language-python=">x = x.to("cpu")y = y.to("cpu")z = x + yprint(z)x = x.to(device)y = y.to(device)z = x + yprint(z)</code></pre><pre><code>tensor([[ 8, 10, 12],        [14, 16, 18]])tensor([[ 8, 10, 12],        [14, 16, 18]], device='cuda:0')</code></pre><pre class=" language-python="><code class="language-python="># shape屬性可以用來查詢張量的形狀x1 = torch.tensor([[1, 2], [3, 4], [5, 6], [7, 8]])  # 建立4列2行的張量print(x1)print(x1.shape)  # 列印張量形狀</code></pre><pre><code>tensor([[1, 2],        [3, 4],        [5, 6],        [7, 8]])torch.Size([4, 2])</code></pre><pre class=" language-python="><code class="language-python="># PyTorch的索引及切片與NumPy相似x1 = torch.tensor([[1, 2], [3, 4], [5, 6], [7, 8]])print(x1[1, 1])print(x1[1][1])print(x1[1, 1].item())  # 純量print(type(x1[1][1]))print(type(x1[1][1].item()))print(x1[:2, 1])  # 切片到第二列，取每列第一個元素print(x1[:2][1])  # 切片到到二列，並取第一列的所有元素print(x1[x1 < 5])  # 印出所有小於5的元素</code></pre><pre><code>tensor(4)tensor(4)4&lt;class 'torch.Tensor'&gt;&lt;class 'int'&gt;tensor([2, 4])tensor([3, 4])tensor([1, 2, 3, 4])</code></pre><pre class=" language-python="><code class="language-python="></code></pre>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> PyTorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World測試</title>
      <link href="/2023/03/21/hello-world/"/>
      <url>/2023/03/21/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre class=" language-bash"><code class="language-bash">$ hexo new <span class="token string">"My New Post"</span></code></pre><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre class=" language-bash"><code class="language-bash">$ hexo server</code></pre><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre class=" language-bash"><code class="language-bash">$ hexo generate</code></pre><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre class=" language-bash"><code class="language-bash">$ hexo deploy</code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
